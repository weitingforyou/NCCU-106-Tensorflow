{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å…©ç¨® RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Recurrent NNï¼šå¾ªç’°ç¥žç¶“ç¶²çµ¡\n",
    "#### RNN çš„è¼¸å‡ºå±¤ o å’Œ éš±è—å±¤ s "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./rnn/rnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./rnn/rnn2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Recursive NNï¼šçµæ§‹æ€§éžè¿´ç¥žç¶“ç¶²çµ¡  \n",
    "éžæ­¸ç¥žç¶“ç¶²çµ¡  \n",
    "ä»¥æ¨¹çµæ§‹åŽ»è™•ç†ä¿¡æ¯ï¼Œè€Œä¸æ˜¯åºåˆ—  \n",
    "è‹¥ä»¥æ¨¹/åœ–çµæ§‹è™•ç†ä¿¡æ¯æ›´æœ‰æ•ˆæ™‚ï¼Œéžæ­¸ç¥žç¶“ç¶²çµ¡é€šå¸¸éƒ½æœƒç²å¾—ä¸éŒ¯çš„çµæžœã€‚ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![](./rnn/5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Gradient Problems\n",
    "\n",
    "* #### exploding gradient problem\n",
    "When activation functions are used whose derivatives can take on larger values, one risks encountering the related exploding gradient problem.\n",
    "\n",
    "* #### vanishing gradient problem\n",
    "In an n-layer network, the gradient (error signal) decreases exponentially with n and the front layers train very slowly.\n",
    "\n",
    "![](./rnn/10.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## LSTM (Long short-term memory)\n",
    "> Invented in 1997  \n",
    "In 2009, LSTM won three ICDAR 2009 competitions in handwriting recognition.  \n",
    "(International Conference on Document Analysis and Recognition (ICDAR))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./rnn/11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./rnn/12-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### cell state\n",
    "the horizontal line running through the top of the diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./rnn/13-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "cell state regulated by gates  \n",
    "The sigmoid layer outputs between 0 and 1.  \n",
    "0 means â€œlet nothing through,â€  \n",
    "1 means â€œlet everything through!â€  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### forget gate layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "forget gate layer looks at $â„Ž_{ð‘¡âˆ’1}$ and $ð‘¥_ð‘¡$, and outputs between 0 and 1.\n",
    "\n",
    "For the cell state $ð¶_{ð‘¡âˆ’1}$, \n",
    "1 represents â€œcompletely keep thisâ€\n",
    "0 represents â€œcompletely get rid of this.â€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./rnn/14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### input gate layer and tanh layer\n",
    "\n",
    "input gate layer decides which values weâ€™ll update.\n",
    "tanh layer creates values, __C_ð‘¡__, that could be added to the state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./rnn/15.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./rnn/16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$ð¶_{ð‘¡âˆ’1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. multiply $ð‘“_ð‘¡$ , how much to forget we decided to forget \n",
    "2. add $ð‘–_ð‘¡Ã—ð¶_ð‘¡$, how much to update we decided to update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Output\n",
    "\n",
    "1. sigmoid layer which decides what parts of the cell state weâ€™re going to output\n",
    "2. put the cell state through tanh and multiply by the sigmoid gate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./rnn/17.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## LSTMï¼šæ¨¡æ“¬äºº\n",
    "* æœ‰è¨˜æ†¶(ç‹€æ…‹)\n",
    "* é¢å°æ–°çš„åˆºæ¿€æ™‚\n",
    "    * è¨˜æ†¶æœƒè¢«æ”¹è®Š\n",
    "    * ç•¶ä¸‹çš„åæ‡‰æ˜¯è¨˜æ†¶èˆ‡æ–°åˆºæ¿€å…±åŒæ±ºå®šçš„\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "\n",
    "words_in_dataset = tf.placeholder(tf.float32, [num_batches, batch_size, num_features])\n",
    "lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "\n",
    "# Initial state of the LSTM memory.\n",
    "hidden_state = tf.zeros([batch_size, lstm.state_size])\n",
    "current_state = tf.zeros([batch_size, lstm.state_size])\n",
    "state = hidden_state, current_state\n",
    "probabilities = []\n",
    "loss = 0.0\n",
    "for current_batch_of_words in words_in_dataset:\n",
    "    # The value of state is updated after processing each batch of words.\n",
    "    output, state = lstm(current_batch_of_words, state)\n",
    "\n",
    "    # The LSTM output can be used to make next word predictions\n",
    "    logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "    probabilities.append(tf.nn.softmax(logits))\n",
    "    loss += loss_function(probabilities, target_words)\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Sequence-to-sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### RNN encoder and decoder\n",
    "\n",
    "### Encoder\n",
    "> to encode a variable-length sequence into a fixed-length vector representation\n",
    "\n",
    "### Decoder\n",
    "> to decode a given fixed-length vector representation back into a variable-length sequence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./rnn/18.png)\n",
    "\n",
    "<p style=\"font-size:12px;color:gray;text-align:right\">To learn the conditional distribution over a variable-length sequence conditioned on yet another variable-length sequence, T and Tâ€™ may differ</p>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
