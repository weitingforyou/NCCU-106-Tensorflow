{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 兩種 RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Recurrent NN：循環神經網絡\n",
    "#### RNN 的輸出層 o 和 隱藏層 s "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./rnn/rnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./rnn/rnn2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Recursive NN：結構性遞迴神經網絡  \n",
    "遞歸神經網絡  \n",
    "以樹結構去處理信息，而不是序列  \n",
    "若以樹/圖結構處理信息更有效時，遞歸神經網絡通常都會獲得不錯的結果。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![](./rnn/5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Gradient Problems\n",
    "\n",
    "* #### exploding gradient problem\n",
    "When activation functions are used whose derivatives can take on larger values, one risks encountering the related exploding gradient problem.\n",
    "\n",
    "* #### vanishing gradient problem\n",
    "In an n-layer network, the gradient (error signal) decreases exponentially with n and the front layers train very slowly.\n",
    "\n",
    "![](./rnn/10.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## LSTM (Long short-term memory)\n",
    "> Invented in 1997  \n",
    "In 2009, LSTM won three ICDAR 2009 competitions in handwriting recognition.  \n",
    "(International Conference on Document Analysis and Recognition (ICDAR))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./rnn/11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./rnn/12-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### cell state\n",
    "the horizontal line running through the top of the diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./rnn/13-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "cell state regulated by gates  \n",
    "The sigmoid layer outputs between 0 and 1.  \n",
    "0 means “let nothing through,”  \n",
    "1 means “let everything through!”  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### forget gate layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "forget gate layer looks at $ℎ_{𝑡−1}$ and $𝑥_𝑡$, and outputs between 0 and 1.\n",
    "\n",
    "For the cell state $𝐶_{𝑡−1}$, \n",
    "1 represents “completely keep this”\n",
    "0 represents “completely get rid of this.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./rnn/14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### input gate layer and tanh layer\n",
    "\n",
    "input gate layer decides which values we’ll update.\n",
    "tanh layer creates values, __C_𝑡__, that could be added to the state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./rnn/15.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./rnn/16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$𝐶_{𝑡−1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. multiply $𝑓_𝑡$ , how much to forget we decided to forget \n",
    "2. add $𝑖_𝑡×𝐶_𝑡$, how much to update we decided to update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Output\n",
    "\n",
    "1. sigmoid layer which decides what parts of the cell state we’re going to output\n",
    "2. put the cell state through tanh and multiply by the sigmoid gate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./rnn/17.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## LSTM：模擬人\n",
    "* 有記憶(狀態)\n",
    "* 面對新的刺激時\n",
    "    * 記憶會被改變\n",
    "    * 當下的反應是記憶與新刺激共同決定的\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "\n",
    "words_in_dataset = tf.placeholder(tf.float32, [num_batches, batch_size, num_features])\n",
    "lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "\n",
    "# Initial state of the LSTM memory.\n",
    "hidden_state = tf.zeros([batch_size, lstm.state_size])\n",
    "current_state = tf.zeros([batch_size, lstm.state_size])\n",
    "state = hidden_state, current_state\n",
    "probabilities = []\n",
    "loss = 0.0\n",
    "for current_batch_of_words in words_in_dataset:\n",
    "    # The value of state is updated after processing each batch of words.\n",
    "    output, state = lstm(current_batch_of_words, state)\n",
    "\n",
    "    # The LSTM output can be used to make next word predictions\n",
    "    logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "    probabilities.append(tf.nn.softmax(logits))\n",
    "    loss += loss_function(probabilities, target_words)\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Sequence-to-sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### RNN encoder and decoder\n",
    "\n",
    "### Encoder\n",
    "> to encode a variable-length sequence into a fixed-length vector representation\n",
    "\n",
    "### Decoder\n",
    "> to decode a given fixed-length vector representation back into a variable-length sequence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./rnn/18.png)\n",
    "\n",
    "<p style=\"font-size:12px;color:gray;text-align:right\">To learn the conditional distribution over a variable-length sequence conditioned on yet another variable-length sequence, T and T’ may differ</p>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
